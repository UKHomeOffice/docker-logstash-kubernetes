filter {
  if "kubernetes" and "docker" in [tags] {
    json {
      source => "message"
      target => "message"
      remove_field => [ "[message][stream]" ]
    }

    # Remove messages bigger than ~20kB in size to stop LS sending bulks which are too big.
    # This calculation is based on default ES max bulk size of 100MB and LS flush_size of 5000.
    truncate {
      fields => "message"
      length_bytes => 20000
    }

    # Grab a timestamp from the actual message, rather than at the point of
    # which events arrive
    if [message][time] {
      date {
        match => ["[message][time]", "ISO8601"]
        remove_field => [ "[message][time]" ]
      }
    }

    # Check if message["log"] looks like json and try to encode it and flatten
    # log fields out into a message field
    if [message][log] =~ /^\{.*\}$/ {
      json {
        source => "[message][log]"
        target => "[message_json]"
        remove_field => [ "[message]" ]
      }
    } else if [message][log] {
      mutate {
        replace => { "message" => "%{[message][log]}" }
      }
    }

    # Extract kubernetes metadata
    kubernetes {
      add_tag => ["kubernetes_filtered"]
    }

    # Clean up
    mutate {
      remove_field => [ "host", "path" ]
    }
  }
}

output {
  if "kubernetes_filtered" in [tags] {
    elasticsearch {
      index => "kubernetes-%ELASTICSEARCH_INDEX_SUFFIX%%{+YYYY.MM.dd}"
      document_type => "kubernetes"
      hosts => [ "%ELASTICSEARCH_HOST%" ]
      validate_after_inactivity => 60
      idle_flush_time => %ELASTICSEARCH_IDLE_FLUSH_TIME%
      flush_size => %ELASTICSEARCH_FLUSH_SIZE%
    }
  }
}
